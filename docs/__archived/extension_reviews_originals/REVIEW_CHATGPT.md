# Tanaka Extension Architecture Analysis and Redesign Proposal
### Analysis of Current Implementation
### Sync Logic and CRDT Integration (Client-Side)

The Tanaka extension’s sync mechanism centers around the SyncManagerWithWorker class, which uses an adaptive sync loop and a Web Worker to handle CRDT operations off the main thread. On each browser tab/window event, a CRDT operation is created and queued via the worker. The design cleverly batches operations with priority: operations are assigned a priority level (CRITICAL, HIGH, NORMAL, LOW) in the worker, and the SyncManager sets a short timeout based on the highest priority in the batch (e.g. 50 ms for CRITICAL) before sending to the server. This ensures urgent operations (like tab closures or window tracking) sync almost immediately, while less critical ones (like URL changes) wait a bit to batch together. The worker also deduplicates operations by key so that only the latest operation per tab is kept (e.g. if a tab’s URL changes rapidly, only the final URL is sent). Overall, the core sync flow – queue ops, batch with priority delays, POST to server, apply remote ops – is logically sound and aligned with eventual consistency goals.

Despite its strengths, there are some areas to improve in code quality and modularity. First, Lamport clock management is slightly duplicated: the main thread SyncManagerWithWorker keeps a Lamport clock (as a BigInt) and increments it for each local operation, and it updates the worker’s copy of the clock as well. The worker then also increments its own clock on queueing the op. This dual increment could lead to the worker’s Lamport clock drifting by an extra tick. In practice the server-supplied clock on sync will override any minor discrepancy, but it’s an unnecessary complexity. Consolidating Lamport clock updates in one place (either only in the main thread or only in the worker) would reduce confusion. Second, the adaptive timing and error-backoff logic could be tuned: currently after a sync error, the next sync is delayed exponentially (5s, 10s, 20s, ... up to 60s), which is good, but if the queue of pending ops grows large during downtime, the code doesn’t yet proactively adjust priority or flush sooner beyond the fixed delays. Notably, the config defines maxQueueSize: 1000 and a queueSizeThreshold: 50, but there is no present logic to flush immediately when the threshold is exceeded – an opportunity for improvement to prevent memory bloat or stale data. On the positive side, the use of BigInt for Lamport clocks and timestamps ensures no overflow and consistent JSON serialization (they convert BigInts to strings when sending to the server and back to BigInt on receiving). This is a robust approach given JavaScript’s limitations with large integers.

From a performance perspective, the current implementation is quite efficient. All heavy work (like CRDT merges) is actually offloaded to the Rust server; the extension mainly enqueues and applies operations. Using a Web Worker here is somewhat forward-looking – for now, queuing and deduping ~1000 operations is not expensive work for the main thread, but if the CRDT logic were to become more complex (e.g. integrating a Y.js document), the worker architecture is already in place. The overhead of posting messages to the worker is minimal for small JSON payloads. In effect, the extension sacrifices a bit of complexity to ensure the UI thread never stalls even under heavy tab churn, which is a good trade-off. One minor inefficiency is that the SyncManager class currently handles multiple responsibilities (op queueing, scheduling syncs, applying remote ops, persistence of state). This all-in-one design works but makes the class quite large and slightly harder to test in isolation. Splitting it into focused components (e.g. a SyncScheduler, an OperationQueue/CRDTManager, and a RemoteOpApplier) would improve modularity without changing functionality.
## Extension Features (Popup, Settings, Background Service, Adapters)

Tanaka’s extension features are implemented across a popup UI, an options/settings page, and a persistent background script, following standard WebExtension structure. The background service is central – it initializes the sync system and listens for events. On startup, the BackgroundService class loads user settings (auth token and base sync interval) from storage and configures the TanakaAPI client and SyncManager accordingly. It then sets up listeners: (a) it installs the tab/window event handlers via syncManager.setupTabEventHandler() to capture browser events, and (b) it registers a message listener for incoming runtime messages (from the popup or settings). This background script runs continuously and serves as the broker between UI actions and the sync logic – a sound design that adheres to the extension’s needs (since content scripts aren’t used here, all logic resides in background and UI pages).

The popup UI is a lightweight interface for the user to control window tracking. It’s built with Preact (functional components with hooks) and uses a small global store for state. On load, the popup initializes by querying the current window and asking the background which windows are tracked. It then shows a checkbox “Sync this window” that is checked if the current window is in the tracked set. Toggling this checkbox sends a message (TRACK_WINDOW or UNTRACK_WINDOW) to the background and optimistically updates the UI state. If the background returns an error (e.g. sync failed), the UI will revert the toggle and display the error. This approach is effective: it keeps the UI responsive and offloads the actual logic to the background via message-passing (using browser.runtime.sendMessage). One limitation is that the popup does not live-update if tracking status changes in the background while it’s open (though in this app, such changes only happen via the popup itself). In the future, the UI might list all tracked windows or tabs, which would require more data from the background. Currently, the background’s MessageHandler returns only an array of tracked window IDs, and the popup expects both windowIds and titles (it checks for titles in the response) – this is a small inconsistency in the protocol (the titles are never sent, thus the popup’s check simply skips populating a title list). This indicates an area for refinement if the UI is extended to display window titles.

The settings page (opened via Firefox’s extension options) allows configuration of the server token and sync interval. It too is a Preact app with its own state store for settings. On mount, it loads the saved settings from browser.storage.local (through the injected IBrowser.localStorage interface) and populates the form. When the user saves, the settings page updates stored values and then notifies the background by sending a SETTINGS_UPDATED message. The background listens for this and, upon receiving it, reloads the settings and reinitializes the SyncManager with the new interval or token. This mechanism works well and decouples the UI from the background: the background is in charge of applying new settings (it calls api.setAuthToken() and potentially restarts the sync loop with a new interval). One could consider directly sharing state or using Firefox’s storage.onChanged events, but the explicit message approach is clear and testable.

The browser API adapters are a notable aspect of the codebase’s design. Rather than using the browser.\* WebExtension APIs all over, Tanaka defines an interface IBrowser that encapsulates the needed browser functionality (tabs, windows, storage, runtime messaging) with proper type annotations. There is a concrete implementation Browser which simply wraps the real webextension-polyfill calls, and importantly, a MockBrowser for testing and a “webapp” mock for running the extension logic outside Firefox. This abstraction is excellent for maintainability: it makes unit tests easy (the tests inject a dummy IBrowser so no real browser API is invoked), and it lays groundwork for cross-browser support – e.g., a Chrome implementation of IBrowser could be provided if needed. The only inconsistency is that a few places bypass IBrowser and use the global API directly (for example, TabEventHandler uses browser.tabs.onCreated.addListener directly inside its setup, rather than going through an IBrowser.tabs.onCreated event emitter). This is minor (because the Browser.tabs.onCreated field is essentially the same event emitter), but ideally all usage would funnel through the adapter for consistency. Overall, the extension feature set is implemented in a logical separation: UI in popup/settings, background handling core logic, and adapters abstracting external APIs.
## Code Quality and Maintainability

In general, the codebase adheres to good TypeScript practices and is thoughtfully structured. There are very few instances of poor type usage – for example, no any types are used except where absolutely necessary for JSON parsing, which are handled via casts and ESLint disable comments. The project even includes a guidance file for AI assistants emphasizing clean code, no unnecessary comments, and making invalid states unrepresentable with types, indicating a strong commitment to code quality. The use of neverthrow’s Result type for error handling is apparent in parts of the code, e.g. SyncManagerWithWorker.sync() returns a Result<void, ExtensionError> and uses ok()/err() to handle success or failure. This pattern makes error propagation explicit, though it isn’t used everywhere (some internal methods still throw or just use try/catch with logging). Consistency could be improved by using the Result type uniformly across the async boundaries.

One potential bug/oversight (as mentioned earlier) is the mismatch in the GET_TRACKED_WINDOWS message response. The background’s MessageHandler returns { windowIds: number[] }, but the Popup expects windowIds and titles arrays in the response (it checks for both before populating its state). In practice, this doesn’t break functionality (the popup only uses the IDs), but it’s a minor inconsistency that could lead to confusion or require adjustment if the UI is enhanced to display window titles. Another edge case bug might occur if the user quickly toggles window tracking on and off: the background will start or stop the sync loop accordingly. The code calls syncManager.start() on every TRACK_WINDOW message, even if sync was already running, but the SyncManagerWithWorker.start() method itself safely handles repeated calls (it checks if already started via the existing timer) or reinitializes appropriately. There’s also logic to stop the sync when no windows remain tracked. This is correct behavior, though one must ensure that stopping the sync doesn’t leave any pending timeouts (the implementation clears them, which is good) and that restarting later works (the code reinitializes state each time, which might risk losing queued ops, but since queued ops only exist when windows were tracked, it’s acceptable to drop them if tracking is turned off). These are subtle points that seem to be handled, but should be verified with thorough testing.

In terms of bad practices, the code is largely free of them. The developers avoid code duplication (for instance, similar event handlers are abstracted in TabEventHandler rather than scattered). One thing to note is that the BackgroundService sometimes mixes dependency injection with manual instantiation. For example, the DI container is configured to provide singletons of WindowTracker, UserSettingsManager, MessageHandler, etc., and to use SyncManagerWithWorker whenever SyncManager is requested. In the BackgroundService constructor, they resolve these from the container. However, when applying new settings, the code manually creates a new SyncManager via constructor instead of asking the container for one. This means the container’s notion of the SyncManager singleton is bypassed. It’s not a critical issue (especially since the container isn’t used elsewhere for SyncManager after initialization), but it is a bit inconsistent. A more container-centric approach would register a factory for SyncManager that takes the latest settings, or simply always construct it manually without registering in the container at all. Deciding on one approach and sticking to it would reduce confusion.

The testability of the code is quite good. The presence of IBrowser and the use of dependency injection mean that components can be tested in isolation by providing mocks (as seen in the numerous Jest tests included). The code organization (with clear separation of concerns) makes it feasible to write targeted unit tests, e.g. there are tests for TabEventHandler ensuring it only reacts to events in tracked windows, tests for UserSettingsManager load/save logic, and tests for the MessageHandler’s various message types. This coverage helps maintainability since future changes that break assumptions will cause test failures. One area to possibly improve is adding more tests for the SyncManager’s adaptive timing and batching (some tests exist for ensuring timers are set and cleared properly, but none for integration with actual worker behavior). Also, if the CRDT logic becomes more complex, corresponding tests for merge correctness would be needed. Overall, the code quality is high, with clear naming, appropriate use of classes/interfaces, and alignment with clean coding principles.
## Performance Considerations

The design targets smooth performance for up to “200+ tabs across devices” and low latency (targeting P95 sync latency ≤ 10 ms). Given that the heavy lifting (applying CRDT merges and storing state) is done server-side, the extension mainly needs to quickly capture events and ship them out. The current implementation achieves this. UI responsiveness is preserved by offloading work to the web worker and by using async browser API calls for everything (Firefox’s browser.\* APIs are promise-based, so they won’t block the UI thread). The use of adaptive intervals means the extension isn’t constantly syncing when idle. When you’re actively interacting with tabs, it syncs every 1 second, which is frequent enough to feel nearly real-time, but when you stop, it drops to 10 seconds to reduce network and CPU usage. This adaptive approach, combined with the exponential backoff on errors, avoids wasteful retries. For example, if the network goes down, it will try at 5s, then 10s, then 20s… up to 60s max, instead of hammering the server. This is a practical balance between timeliness and resource use.

Operation batching also contributes to performance. If 10 tabs are opened in quick succession, Tanaka will likely send them in one batch instead of 10 separate requests, because the first operation starts a ~200 ms timer (HIGH priority for tab open) and subsequent operations piggyback on that batch. This amortizes network overhead and ensures the server can apply a set of related ops together. Deduplication, as discussed, prevents redundant work. For instance, if a tab’s URL changes 5 times rapidly (think auto-redirects or typing in the address bar), the extension’s queue will end up sending only the final URL change operation. This not only reduces network chatter but also reduces load on the server and on other clients that would receive those ops.

Memory-wise, the extension is lightweight. The in-memory operation queue (in the worker) is capped at 1000 ops by config, and each operation is just a small JSON object. In a pathological scenario, 1000 queued operations might be, say, ~500 bytes each -> ~500 KB of memory – trivial for modern systems. The rest of the state (Lamport clock, device ID, tracked windows) is tiny. There is no large in-memory model of all tabs (the extension queries the browser for current tabs as needed). One performance consideration is the initial sync for a new device: if the server has a long history of operations (say thousands of ops from other devices), and a new device with lastSyncClock = null starts syncing, the server will dump all those ops to the client. Applying them one by one via the current applyRemoteOperations loop (which creates/moves/updates tabs sequentially) could be slow and possibly overwhelm the browser. In practice, a user likely wouldn’t have thousands of tabs operations unless using Tanaka for a long time. Nonetheless, a possible enhancement is to have the server provide a compacted snapshot of current state for new clients (so they create the current set of windows/tabs directly, instead of replaying the entire log). This isn’t implemented now, so initial sync could be a bit heavy – something to monitor.

Worker communication overhead is minimal here, but if the CRDT logic evolves to include transferring large data (like a full Y.js document state), the current approach (structured cloning of messages) might become a bottleneck. In such a case, we might consider using transferable objects or shared memory (if supported) for the CRDT state, or employ a library like Comlink to call worker functions directly without manual message passing. At the moment, though, none of these are necessary – the payloads are small JSON operations and the message latency (a few microseconds) is dwarfed by network and disk I/O latencies.

In summary, the extension as written should meet its performance goals. The combination of adaptive sync intervals, batching, and offloading ensures it “has smooth UI with no blocking operations”. Testing with a large number of tabs (e.g. 200 open tabs and frequent changes) would be wise to verify there are no memory leaks or event handling slowdowns. One potential improvement could be to incorporate some instrumentation – for example, measuring how long applyRemoteOperations takes or how big the operation queue gets – and log warnings if thresholds are exceeded. This would help in profiling and optimizing future versions, especially as new features are added.
## Architecture and Modularity

Tanaka’s architecture follows a fairly classic layered approach for a browser extension, with some elements of Clean Architecture, but it could be made more explicit. We have: a background service layer that orchestrates everything, a domain layer (sync/ module containing sync logic, window tracking, message handling), and infrastructure layers (the browser/ adapters, the api/ client, the Web Worker for CRDT). The UI (popup and settings) form a presentation layer on top. This separation is already evident in the project structure and is even described in the documentation – for example, the docs highlight a background service handling events and sync, UI components (popup/settings), and message passing between them. Moreover, the guidelines mention plans for a “service layer with dependency injection” and a “repository pattern for data access”, indicating the intent to further decouple business logic from direct API usage.

Currently, there are a few breaches in layering: the SyncManager directly calls the browser APIs when applying remote operations (e.g. calling browser.tabs.create/update inside applyOperation). In a pure Clean Architecture sense, the sync logic (domain) would not directly know about the browser (an external system); instead it would call an interface like TabController which an infrastructure component implements. Here, SyncManagerWithWorker plays a dual role – it contains domain logic (when to sync, how to deduplicate) and also contains knowledge of the browser (how to actually open/move tabs). This works (and is sometimes pragmatic in an extension where the “domain” is tightly tied to browser behavior), but it reduces testability of the sync logic in isolation. We see some attempts to abstract – e.g., WindowTracker encapsulates the set of tracked window IDs and its track()/untrack() are used by both UI (via messages) and SyncManager (applying remote ops). This is a good abstraction: window tracking concern is localized to that class. Similarly, UserSettingsManager abstracts reading/writing settings to storage. Extending that idea, one could introduce an abstraction for “tab operations applier” so that SyncManager just generates the operations but a different component knows how to apply them to the actual browser.

The dependency direction mostly points outward, as it should in clean architecture: the domain (SyncManager, WindowTracker) is used by the background, and the background also holds the infrastructure (Browser, TanakaAPI) which it injects into the domain. The use of a DI container (tsyringe) ensures that, for example, the MessageHandler receives a WindowTracker and SyncManager in its constructor without itself constructing those. This is an inversion of control that makes testing easier (you can swap out those dependencies). One improvement would be to define interfaces for some of these dependencies (for instance, define an interface ISyncManager that the MessageHandler depends on, rather than the concrete class). However, given the scale of the project, this might be an unnecessary abstraction at the moment.

Another architectural aspect is horizontal layering within the domain. The sync/ module contains multiple concerns: tab event handling, message handling (for UI commands), the sync logic itself, and window tracking state. They do interact (e.g. SyncManager uses WindowTracker, MessageHandler uses both, etc.), but each could be thought of as a service on its own. The coupling is low – e.g., WindowTracker is mostly independent (just a set with helper methods), which is good. The MessageHandler is essentially implementing an application boundary (it’s the point where external input from UI is translated to domain operations) – this is a “port” in Ports & Adapters terminology. The code for MessageHandler is straightforward: it validates the message shape and then delegates to either windowTracker or syncManager methods. This is a clean separation of the command handling from the UI itself (the UI never calls WindowTracker or SyncManager directly, always goes through the message interface). Such a pattern could be extended in the future (e.g., if a UI page or a content script wanted to invoke other commands, you’d add cases to MessageHandler).

In summary, the current architecture is logical and mostly conforms to clean architecture principles: entities (like tracked windows, CRDT operations) are simple data, use-case logic (like “start syncing when a window is tracked, stop when none remain”) is in the background service & MessageHandler, and interfaces (like IBrowser, TanakaAPI) abstract the external world. There is room to enforce these boundaries more strictly (introducing a clearer domain model, using interfaces for applying ops), but even as-is, the code is modular. The fact that future plans include a P2P sync mode, cross-browser support, and collaborative featuresmeans the architecture will need to be flexible. Right now, adding those would require changes (for example, P2P sync might involve direct extension-to-extension communication, which would be a new layer). The good news is that the fundamental separation (UI vs background vs sync logic vs server API) is in place, which will make such enhancements easier to incorporate without a total rewrite.
## Proposed Refactor and Architecture Improvements

Based on the analysis above, a full architecture redesign is not necessary – the project is already well-structured – but there are several targeted improvements that can make the system more robust, maintainable, and future-proof. Below is a proposal that introduces clearer domain boundaries, better separation of concerns, and modern best practices, formatted as a structured plan for senior engineers to implement.

1. Establish Clear Domain Boundaries and Layered Modules

We propose reorganizing the codebase into distinct layers to enforce separation of concerns:

    Core Domain Layer (domain/): This will contain the business logic and state management that is independent of browser APIs or UI. Key modules here:

        WindowTrackingService – Manages which windows are being synced (essentially the current WindowTracker). It would expose methods like trackWindow(windowId), untrackWindow(windowId), isTracked(windowId), and maintain the set of tracked IDs in memory (and perhaps persist it if needed). No direct browser calls, just logic and internal state.

        SyncService (TabSyncManager) – Responsible for handling the queue of CRDT operations and the sync scheduling loop. This is an evolution of SyncManagerWithWorker but cleaned up. It should not directly manipulate browser tabs; instead it focuses on producing and consuming operations. It uses an Operation Queue (see below) and interacts with a SyncRepository interface to fetch/post operations (implemented by the Tanaka API client).

        OperationQueue / CRDTManager – This could be an abstraction for what the web worker currently does. For now it might wrap the worker communication, but we can define it as an interface in the domain layer: e.g., IOperationQueue.enqueueOp(op), deduplicateOps(), etc. The SyncService uses this interface, unaware whether it’s a background thread or in-process. This isolates the SyncService from the mechanics of deduplication and allows swapping the implementation (for instance, use a different CRDT library or move everything in-thread).

        TabStateApplier interface – Define an interface in the domain layer for applying a high-level tab operation to the local browser state. For example, applyOperation(op: CrdtOperation): Promise<void>. The domain logic will call this when it needs to effect a change (e.g., apply a remote “upsert_tab” or “close_tab” operation locally). By having an interface, we decouple what needs to be done from how the browser does it. This is essentially the inverse of generating operations. It could also handle validation (ensuring, say, we don’t open duplicate tabs).

The domain layer should not depend on any Firefox/Chrome APIs or UI specifics – it deals with plain data and abstract operations. Types like CrdtOperation, SyncRequest/SyncResponse models, etc., live here (as they are basically data structures). This layer can be well-unit-tested by mocking out the interfaces for the outside world.

    Infrastructure Layer (infrastructure/ or reuse browser/ and api/ modules): This layer implements the interfaces defined by the domain, using real APIs:

        BrowserAdapter (IBrowser implementation) – Continue to use the Browser class as is for direct API calls. We might extend it or add separate classes to implement the TabStateApplier interface. For instance, a BrowserTabApplier class in this layer will implement applyOperation(op) by translating it to the appropriate browser.tabs.update/create/remove calls. This would largely move the logic currently in SyncManagerWithWorker.applyOperation() into this adapter. The SyncService (domain) would hold a reference to a TabStateApplier but only know it as an interface.

        TanakaAPI Client – The existing TanakaAPI class already encapsulates the HTTP calls and is written in a generic way. We will treat it as part of the infrastructure layer implementing a ISyncRepository interface. For example, define ISyncRepository with methods sync(request): Promise<SyncResponse> and perhaps checkHealth(). TanakaAPI already matches this shape (with .sync() and .checkHealth() methods), so we might simply have TanakaAPI implement ISyncRepository. This abstracts the server communication behind an interface, which is useful if we later introduce an alternative sync mechanism (say, a peer-to-peer service).

        Storage (Settings/State) – The UserSettingsManager can be considered part of infrastructure (since it wraps browser.storage). We can keep this as is, but possibly rename it to SettingsRepository. It provides domain with needed config (auth token, base sync interval). If we need to persist other state (like last sync timestamp, device ID, etc.), we might create a small SyncStateRepository that uses browser.storage.local to save those (right now, SyncManager does it directly; we can move that into an infrastructure class). This way, the SyncService can call an interface ISyncStateStore.saveClock(clock) without knowing how it’s saved.

        Web Worker – The CRDT web worker (crdt-worker.ts) is a special kind of infrastructure. We will preserve it, but we can simplify its interface by using a library like Comlink (which creates proxies to worker functions). Instead of manually posting messages and handling IDs, we would expose functions from the worker (like enqueue(op), getDeduplicatedOps()) and call them directly. Comlink would handle the message passing under the hood. This results in cleaner code – e.g., await worker.enqueue(op) instead of the current sendMessage('queue', op). If introducing a library is undesirable, we can at least refactor CrdtWorkerClient to reduce boilerplate (for instance, using async/await instead of manual Promise construction with timeouts). The key is that the OperationQueue interface in domain doesn’t care how it’s implemented. It could be an in-memory JS array (for simplicity in a future version), or the existing worker approach. We should keep the worker for heavy load, but hide its complexity behind the interface.

    Application/Background Layer (background/ or app/): This is where everything is wired together. The BackgroundService can be refactored to primarily compose the domain and infrastructure pieces. It will:

        Initialize the DI container (or manual wiring) with the concrete implementations. For example, instantiate TanakaAPI with server URL from config (as done now), instantiate Browser adapter, then instantiate WindowTrackingService and SyncService from the domain, injecting the Browser adapter, API client, etc. (Using tsyringe, we can register Browser as IBrowser, register TanakaAPI as ISyncRepository, etc., and register the domain services as singletons that depend on those).

        Set up the tab event listeners. Currently SyncManager.setupTabEventHandler() creates a TabEventHandler and starts it. In the new design, we might not even need a separate TabEventHandler class; the background can directly subscribe to browser.tabs.onCreated, etc. However, keeping a dedicated class is fine – we can move it to the infrastructure layer (since it directly uses browser.tabs events) and let it call into the domain. For example, TabEventListener (infra) listens for events and calls WindowTrackingService.trackWindow(id) or SyncService.queueOperation(op) as appropriate. We will ensure it only listens to events for tracked windows, similar to current logic. This invert control: instead of SyncManager pulling events, the background pushes events into the SyncService.

        Set up the message (commands) listener for runtime messages. The MessageHandler can remain in domain as a coordinator of user commands, but likely it fits better as part of the application layer since its job is to bridge UI and domain. We could keep the current approach: the background’s onMessage listener simply passes the message to MessageHandler.handleMessage(), which in turn calls WindowTrackingService and SyncService methods. One improvement is to define a clear schema for messages (they did via a type guard asMessage), and potentially expand MessageHandler to handle additional commands (like a future “LIST_TABS” command to support a richer UI, etc.). The output of MessageHandler would remain simple { success: true } or { error: '...' } objects, which the UI already expects.

With these layers, we achieve a Clean Architecture style: domain services know only interfaces, and the background ties domain to infrastructure. For instance, the SyncService in domain might have a constructor like constructor(queue: IOperationQueue, repo: ISyncRepository, tabApplier: TabStateApplier, options: SyncOptions) and the background will instantiate it by passing in new CrdtWorkerQueue() (implements IOperationQueue), the TanakaAPI client (implements ISyncRepository), and the BrowserTabApplier (implements TabStateApplier). The SyncService doesn’t know that the queue is a Web Worker or that the repo is doing HTTP – it just calls methods. This makes it easier to test SyncService with a stubbed queue or a fake API (we could simulate network errors, etc., in unit tests without needing a real server). It also will make future changes (like using a different CRDT library or adding offline storage of ops) localized to the infrastructure implementations without affecting core logic.

Proposed file structure:

```
src/
├── domain/
│ ├── sync-service.ts (SyncService class)
│ ├── operation-queue.ts (interface IOperationQueue + maybe simple impl)
│ ├── window-tracking.ts (WindowTrackingService class)
│ ├── message-handler.ts (if kept in domain/app layer)
│ ├── types.ts (CrdtOperation, SyncRequest/Response, etc.)
│ └── interfaces.ts (TabStateApplier, ISyncRepository, ISyncStateStore, etc.)
├── infrastructure/
│ ├── browser/
│ │ ├── browser-adapter.ts (Browser class implementing IBrowser)
│ │ ├── tab-applier.ts (BrowserTabApplier implementing TabStateApplier)
│ │ └── ... (storage, runtime as needed)
│ ├── sync/
│ │ ├── tanaka-api.ts (TanakaAPI implementing ISyncRepository)
│ │ ├── crdt-worker-client.ts (could remain similar or replaced by Comlink proxy)
│ │ └── crdt-worker.ts (Web Worker code, unchanged in structure)
│ └── storage/
│ └── settings-repo.ts (UserSettingsManager as SettingsRepository)
├── app/
│ ├── background.ts (BackgroundService orchestration)
│ └── listeners.ts (setup of tab event listeners and runtime message listener)
├── ui/
│ ├── popup/...
│ └── settings/...
└── di/container.ts (DI setup if using tsyringe)
```

This is a rough layout. The key change is introducing a domain directory to isolate pure logic, and moving browser-specific event handling to either infrastructure or app. In practice, since WebExtensions require the background script to register listeners in the global scope, our background.ts (in app) will import the necessary services and set up listeners on browser.\*. Those listeners then call into domain services. For instance, browser.tabs.onCreated.addListener(tab => { if (windowTracking.isTracked(tab.windowId)) syncService.queueTabUpsert(tab); }). We might still use a helper class for organizing these listeners (similar to current TabEventHandler but simplified as functions or an observable pattern).

This re-architecture will make the code more flexible. For example, if we want to add cross-browser support, we just implement a new BrowserAdapter for Chrome (which might differ in how it uses chrome.\* API or some polyfill differences) and register that when running in Chrome. The domain and rest of the app don’t change. If we later add a “snapshot sync” feature, we could extend ISyncRepository with a method fetchSnapshot() and implement it in TanakaAPI (server would need an endpoint). The SyncService could then decide to use snapshot on first sync instead of operations. All this can happen without tangling up UI or background logic. 2. Sync Logic Improvements and CRDT Handling

For the sync process, the rewrite will preserve the adaptive and batching behavior but with cleaner abstraction and potential enhancements:

    Single Source of Clock: Adjust the Lamport clock handling so that only the SyncService (domain) maintains the authoritative clock value. The worker should not independently increment Lamport timestamps. In practice, we can remove the worker’s internal lamportClock or just use it for debugging. The SyncService will increment its clock for each local op and include that in the operation data (as it does now). It will also update the worker’s state by sending the new clock, but the worker will refrain from bumping it further. This avoids the double-increment issue observed in the current code. Essentially, think of the worker as a dumb queue that doesn’t need to know about Lamport time except to receive it for reference. The core Lamport logic (max with remote clock, increment on local op) stays in SyncService, which is easier to reason about and test.

    Enforce Queue Size Limits: Implement the queueSizeThreshold logic that currently exists only as a config constant. For example, if the worker reports (or the OperationQueue interface can have) a current queue length, the SyncService can decide to not wait the full adaptive interval if the queue is growing too large. If queueLength > queueSizeThreshold (say >50 ops), we could trigger an immediate flush (similar to treating it as a high priority batch). This ensures we don’t accumulate too many operations during high activity. The maxQueueSize (1000) can be enforced by dropping old ops or refusing new ops (though dropping would risk losing data). A better approach: if maxQueueSize is reached, treat it as critical and force an immediate sync, resetting the backoff. These conditions should rarely trigger, but building them in adds robustness for edge cases.

    Structured CRDT Operations: Continue using the JSON operation format (e.g. upsert_tab, close_tab) as it’s human-readable and flexible. We might consider adding compression or merging of ops if multiple operations are generated within a very short time for the same tab. The documentation suggests “enhanced CRDT protocol with compression” is on the roadmap. With our OperationQueue abstraction, we could implement a more advanced deduplication/compression scheme in the future (for example, merging a sequence of URL changes into one operation). Right now, the deduplicate-on-send approach suffices; just keep this extensible.

    Web Worker Interface: Modernize the worker communication. As noted, adopting Comlink or a similar RPC library would allow us to treat the worker’s functions as promises, eliminating manual ID management. If we prefer not to add a dependency, we can still refactor CrdtWorkerClient to use async/await and maybe an AbortController for timeouts. For instance, instead of a fixed 5s timeout for all worker requests, we could make it configurable or context-based (the queue operation might not need such a long timeout, whereas deduplicate might if it were heavy – currently all are quick). Improving this will simplify the code and reduce the chance of bugs (like mismatched IDs or unhandled rejections). Additionally, ensure the worker is properly terminated when the extension unloads or the background is shutting down. The current code calls worker.terminate() when stopping the SyncManager, which is good. We might also handle worker errors globally (the onerror handler does a debug log but perhaps could notify the SyncService to fallback to a non-worker mode if the worker crashes).

    Apply Operations via Adapter: Remove the browser-specific logic from SyncService when applying remote ops. Instead of the big switch-case inside applyOperation, the SyncService (or a smaller collaborator in domain) would interpret the CrdtOperation and call the TabStateApplier interface. For example, upon receiving an upsert_tab operation, SyncService might call tabApplier.upsertTab(tabId, windowId, url, title, index, active). The BrowserTabApplier (in infrastructure) implements upsertTab to perform the actual API calls (checking if the tab exists and updating or creating it). This separation yields two benefits: (1) The domain logic remains focused on when to apply something and what to apply, rather than how to do it – making it easier to adjust for other browsers or to simulate in tests. (2) It consolidates all browser API interactions in one place (BrowserTabApplier), which can handle edge cases consistently (like Firefox not supporting certain operations – e.g. set_window_focus is a no-op and can be logged once in the adapter, rather than baked into domain).

    Error Handling and Retries: The SyncService will continue to use the Result pattern for sync attempts. On a failed sync (network issue, server down), it should keep the unsent operations in the queue. The current logic does this by re-queueing the ops on failure – in the new design, since we might not flush the queue until success, we could simplify: don’t clear the queue until a successful POST returns. If the POST fails, the ops are still in our OperationQueue (so no need to push them back). We will, however, implement a retry with backoff similar to now. This might be built into the TanakaAPI (it has a retry mechanism with a circuit breaker option). We should ensure the SyncService doesn’t double-retry if the API client is already retrying internally. Probably we can disable the internal retry (enableRetry: false in TanakaAPI options) and instead manage retries at the SyncService level for clarity – or vice versa. A consistent strategy would be to have TanakaAPI do immediate retries for transient failures (e.g., up to 3 quick attempts for a single call) while SyncService manages longer-term backoff between sync rounds.

    Lamport Clock Sync with Server: Continue to update the local clock with the server’s response clock on each sync. This guarantees monotonic clocks across devices. We should also update the persisted lastSyncClock in storage via a SyncStateStore. In the new design, after a successful sync, SyncService can call something like syncStateStore.saveLastSyncClock(lastSyncClock). This ensures that if the extension is reloaded or the browser restarts, we know where we left off. The device ID is similarly persisted. We’ll keep these in local storage as strings (as done now). One enhancement: if the extension ever supports multiple users or profiles, the device ID generation (currently a random string with timestamp) could incorporate something unique (like a user ID or profile name), but since Tanaka is personal use, this is not a pressing concern.

3. Improved Extension Feature Integration and State Management

On the popup and settings UI side, the current architecture is mostly fine but we can make a few improvements to align with the new domain separation:

    The popup currently uses a global store (trackedWindows map and some signals) to manage state, and explicitly requests data from background on mount. We can keep this approach, but if the domain were accessible, an alternative could be to have the background push state changes. For instance, when the user tracks or untracks a window (either via popup or another UI), the background could send a runtime message to any open popup script like { type: 'TRACKED_WINDOWS_UPDATED', windowIds: [...], titles: [...] }. The popup could listen for such messages and update its state. This would allow real-time reflection of state if multiple windows with popups open, etc. However, this added complexity may not be necessary now. It’s something to consider for a more dynamic UI (especially if we add a page that shows all tabs across devices – that would require continuous updates).

    We should ensure the Settings page properly handles invalid input and feedback. The current design disables the save button while saving and shows a message on success or error. That’s good UX. One thing to check is that changing the sync interval indeed propagates – with our new SyncService, when SETTINGS_UPDATED arrives, the background will call something like syncService.restart(newInterval) or recreate the SyncService with new options. Ideally, we can avoid recreating the whole SyncService (to not lose any queued ops). Instead, we could add logic in SyncService to adjust its base interval and reschedule the next sync accordingly. For simplicity, though, restarting it (stop old, create new) is okay, given that tracking state is preserved separately and the queue is empty at idle times. We just need to document that changing sync interval might cause an immediate sync flush (since we’ll re-init).

    The Message passing contract between UI and background should be clearly defined in one place (maybe a TypeScript enum or union of message types). We already have Message and MessageResponse types in core or sync module. We will continue using those, maybe moving them to the domain types. On the background side, the MessageHandler will be updated to use the new domain services. For example, on TRACK_WINDOW it now calls windowTracking.trackWindow(id) and then syncService.startSyncing() if not already running. Actually, we can simplify: rather than starting and stopping the SyncService manually on each track/untrack, we can make SyncService always running but simply do nothing if no windows are tracked. Another approach is keep it lazy as now – start on first track, stop on last untrack. Either is fine. If we keep the lazy behavior, MessageHandler will handle it as currently: if trackWindow, then call track and if the sync loop isn’t running, call syncService.start() (which kicks off the periodic sync timer). If untrackWindow, call untrack, and if no windows left, syncService.stop() (which stops the timer and perhaps clears the op queue). These methods will exist on SyncService. In effect, our SyncService will gain an awareness of whether it’s active or not (we could fold that logic into WindowTrackingService as well – e.g., WindowTrackingService emits events when it becomes non-empty or empty, and the BackgroundService toggles sync accordingly). Decoupling that might be elegant: let WindowTrackingService have a listener/callback for “tracked count changed”. If count goes from 0->1, start sync; 1->0, stop sync.

    State management libraries: The UI currently uses Preact signals (it looks like simple observable state via store/popup.ts etc.). This is fine. We don’t necessarily need Redux or others for such a small UI. However, if the UI were to grow (say, a page showing a list of all open tabs across devices), a more structured state management might help. Preact’s built-in signals or React’s Context + useReducer could manage that. For now, I’d suggest continuing with the lightweight approach but ensuring that state stores are documented (the contract that trackedWindows is updated via messages from background, etc.).

    UI Components modularization: The popup consists of PopupApp and a WindowTracker component, which is appropriate. If we foresee adding more to the popup (like a list of tabs or a “Sync Now” button), we can keep those as separate components and possibly introduce a routing or tabbed interface in the popup. The settings page is well-isolated. We should make sure to handle edge cases like invalid token format (perhaps just any non-empty string is fine) or sync interval out of range (the UI helper text suggests 1-60 seconds, but there’s no validation currently beyond it being a number). We could clamp the value or show an error if out of bounds.

4. Code Quality Enhancements and Patterns

Several coding practices and patterns can be employed to improve maintainability:

    Dependency Injection & Inversion: Continue using tsyringe to inject dependencies, but standardize its usage. For instance, we will remove the registration of SyncManager as a singleton since we plan to instantiate it with dynamic config. Instead, we might register a factory: container.register<SyncService>('SyncService', { useFactory: () => new SyncService(/*...*/)} ). However, constructing the SyncService involves reading settings (async). It might be simpler to not use DI for SyncService at all and manage it in BackgroundService.initialize() manually (which is effectively what we did before). The container is still very useful for singletons like WindowTrackingService, SettingsRepository, and for providing the same TanakaAPI instance everywhere. We can leverage tsyringe’s scoping if needed: e.g., if we wanted to reset the SyncService on settings change, we could resolve a fresh instance from a child container. But likely, manual re-init is fine.

    Neverthrow Result usage: We should standardize error handling. One approach is to wrap all async boundaries in Result types (for internal logic) and only throw/log at the top level (background or UI). For example, SyncService.sync() returns a Result<void, SyncError>. MessageHandler could capture any failure from syncService and return an { error: message } response. This would avoid unhandled promise rejections. In the UI, we can use .catch() on browser.runtime.sendMessage calls or use the fact that background sends back an error object. Currently, the UI does const response = await sendMessage and checks if response.error field exists – that’s a fine pattern. We’ll keep that.

    Type strictness: Turn on full strict mode if not already (very likely it is on). Ensure noImplicitAny, noUnusedParameters, noUnusedLocals etc. are enabled to catch issues early. The code already is pretty strict about types. We will remove the few @ts-expect-error or eslint-disable usages by solving the underlying issue. For instance, the browser.tabs.move returns Tabs.Tab | Tabs.Tab[] (because you can move multiple tabs in Chrome). They cast it to Promise<Tabs.Tab> in BrowserTabs.move(). We can instead adjust the type definition or add a runtime check to always treat it as moving a single tab. These little things improve clarity.

    Comments and Documentation: Continue the philosophy of self-documenting code with good naming. We’ll add doc comments for public methods in the domain services, since these are crucial (e.g., explaining what SyncService.start() does vs syncNow() vs internal scheduling, etc.). This helps future contributors. We can also update the ARCHITECTURE.md doc to reflect the new design (the ASCII diagram might change slightly to show the new layering, and we can add a sequence diagram of how an event flows from browser -> domain -> server and back).

    Design Patterns: We are effectively implementing Ports and Adapters (Hexagonal architecture) by introducing interfaces for the browser and network interactions, which is a recommended pattern for such applications. We’ll also use the Repository pattern for storage as planned (SettingsRepository for extension settings, possibly a repository for persisted sync state if needed). Additionally, we can use the Observer pattern or Pub/Sub for some parts: for instance, WindowTrackingService could expose an event or callback when the set of tracked windows changes. The background could subscribe to that to trigger UI updates or log events. This decouples components further. Using a small event emitter (could reuse Node’s EventEmitter or a simple callback list) in the domain is fine as long as it’s not too heavy.

    Testing & Stubbing: With clearer interfaces, writing tests becomes easier. We should create unit tests for SyncService in isolation by providing a fake OperationQueue (that just collects ops without a worker) and a fake SyncRepository (that maybe simulates a server by echoing back operations or introducing delays). Then test scenarios: syncing with no ops (should do nothing), syncing with some queued ops (should call API and clear queue), error from API (should retry/backoff), remote operations application (should call TabApplier with correct ops). We can also test WindowTrackingService easily (it’s just a Set). Ensuring these core pieces via unit tests will guard against regressions when tweaking the logic.

5. Performance and Tooling Enhancements

To ensure the refactored extension meets performance expectations and is observable in production, we suggest:

    Benchmark/Stress Testing: Create a script or manual procedure to open, close, and move a large number of tabs (100+), perhaps via automation or by instrumenting the extension to generate dummy ops. Measure the sync loop timing and memory usage. We can use Firefox’s about:performance or about:debugging to monitor the extension’s CPU usage under load. This isn’t a code change, but a practice to adopt. We could incorporate a simple benchmarking in the extension’s development mode: for instance, a hidden command or a special debug popup that triggers a flurry of operations and logs timings. Given the target of P95 ≤ 10ms for sync latency, we might log the actual network latency and the end-to-end time for each sync cycle (the code already measures request time). These metrics could be reported in the console or to a dev UI.

    Resource Cleanup: Ensure that on unloading the extension or disabling tracking, we free up resources. The current design stops the timers and terminates the worker on syncManager.stop(). We will keep that. Also ensure to remove any event listeners we added (the TabEventListener cleanup currently removes listeners on window removed events, etc., though in our design if we add directly to browser events, we can remove them when we stop syncing). Memory leaks are unlikely given the scope, but careful removal of listeners and nulling out references helps.

    Modern Build and Linting: The project already uses Rspack for bundling and has ESLint/Prettier and pre-commit hooks (as indicated by the .gitignore and docs). We should maintain those. Possibly enable TypeScript incremental builds for faster dev cycle (if not already configured). Rspack likely handles hot reload with the React Refresh plugin configured.

    Strict Content Security Policy Compliance: If we introduce new libraries (like Comlink), ensure they don’t require eval or unsafe blobs that might conflict with Firefox’s extension CSP. Comlink can work with workers fine. Just something to double-check.

    Observability: For a production-quality system, adding some form of logging or monitoring is useful. We can introduce a lightweight logging service that funnels logs to console.debug or console.error (already using debugLog, debugError helpers). In a future enhancement, these could be toggled by a verbosity setting or even sent to a remote logging service for troubleshooting (since this is personal, probably not needed). The docs mention metrics and tracing in the roadmap – if that becomes relevant (e.g. counting how often sync happens, how many ops synced, etc.), our new architecture should make it easy to add counters in the domain layer. For instance, SyncService could maintain counters for “opsSent” or measure “lastSyncDuration”. These could be exposed via a debug message or a hidden UI for advanced users.

    Tooling for Development: We could incorporate web-ext CLI for running the extension in a browser during development, which auto-reloads on code changes. Also, since the extension is written in TypeScript/Preact, making sure source maps are enabled in Rspack allows us to debug the TS code in the browser inspector.

    Cross-Device Testing: The ultimate test of our sync logic is using two browsers (or browser profiles) and ensuring operations converge properly. We should test scenarios where Device A performs actions while Device B is offline, then Device B comes online (should apply all missed ops in correct order), and conflict scenarios (if both devices open a tab with the same ID, our CRDT design likely uses unique IDs per device, so no conflict; but if both rename the same tab at once, last Lamport wins). These are more integration tests than unit tests. While not strictly architecture, building a small test harness or using the server’s test endpoints could help validate that our refactored client works as intended.

6. Future-Proofing and Modernization

Lastly, consider some modernization and future-proofing steps that can be integrated alongside the refactor:

    Type-Generated API Models: If not already, use tools to ensure the TypeScript types for operations match the Rust server’s model. It was mentioned that types might be generated via ts-rs for Rust models. Incorporating that into the build (so that when the server’s CRDT schema updates, the extension picks it up) will prevent drift. In our domain types, ensure all possible operation types are accounted for in the union (so the default case in applyOperation never actually happens except for truly unknown types).

    State Synchronization and Management: As the system grows, we might implement a unified state management in the background for all synced tabs. Right now, the extension queries the browser for current tabs when needed (e.g., on remote upsert, it does browser.tabs.query to see if tab exists). We could maintain an in-memory representation of the tabs state in the background (like a mini database of tracked tabs). The CRDT operations would be applied both to the real browser and this in-memory model. This would allow more sophisticated features like a popup that lists all tabs (without querying every time) or the ability to resolve conflicts with more context. Implementing this is a non-trivial addition, but our refactored architecture sets the stage: the domain could have a TabRepository that stores tab metadata (id, title, URL, windowId, etc.) whenever an op is applied. This is similar to how the server maintains a materialized state table. For now, we won’t implement it, but the architecture should not preclude it. By keeping domain logic separate, adding such a repository later would mostly affect the domain layer.

    Strict Mode and Security: Run the extension in Firefox’s about:debugging with warnings enabled to catch any potential synchronous operations or deprecated API uses. Also test with Firefox ESR or strict settings to ensure no issues with our use of Web Workers or polyfills. Possibly add a Content Security Policy if needed (though not needed if we aren’t loading remote scripts).

    Documentation and Onboarding: Update the ARCHITECTURE.md to reflect the new module structure and design decisions, so future contributors (or even open-sourcing the project) can understand the intent. Emphasize how the ports and adapters approach is used: e.g., “The SyncService (domain) interacts with the browser via the TabStateApplier port, which is implemented by BrowserTabApplier using WebExtension APIs.” This aligns with Clean Architecture and will make the code easier to maintain and extend.

    Continuous Integration (CI): If not already set up, add CI workflows to run tests and lint on each commit. Given the emphasis on pre-commit hooks and no-verify rules in the guidelines, the project maintainers value a clean CI. We should ensure our refactor doesn’t break any lint rules or tests; likely we’ll update tests as needed to match new class/method names but keep similar coverage.

By implementing the above proposals, we expect the following outcomes:

    The sync logic becomes more modular (separating scheduling, queuing, and applying ops) and easier to adjust or optimize. The CRDT integration remains effective, with less risk of logic duplication or race conditions around Lamport clocks.

    The extension features (popup and settings) remain functionally the same but backed by a more robust message handling and state management. Developers can add new commands or UI features with minimal changes to existing code.

    Code quality improves via clearer interfaces and division of responsibilities. Each piece of logic has a single home (e.g., all tab API calls in one adapter, all state in one service), making the codebase easier to navigate.

    Performance stays top-notch due to batching and worker usage, and potential bottlenecks are mitigated by threshold-based flushes and reduced overhead in worker messaging. We also pave the way to handle heavier workloads (like future P2P sync or local state storage) by having the right abstractions in place.

    The architecture adheres closely to Clean Architecture principles, as was intended in the project’s roadmap. This means the project will be easier to maintain and extend for future versions (v1.0, v2.0 features like cross-browser support or collaborative tab sharing) because the core business logic is isolated and the framework details (browser, network) are swappable.

In conclusion, the Tanaka Firefox extension will benefit from these refactoring measures by becoming more robust against edge cases, simpler to test and reason about, and adaptable to new requirements – all while retaining its efficient synchronization of browser tabs across devices. The proposal above serves as a blueprint for the team to implement a cleaner architecture without disrupting the existing functionality or performance guarantees of Tanaka.

Sources:

    Tanaka Project Documentation and Code – Architecture overview and current implementation details, etc. The references above illustrate the existing design which informs these recommendations.
